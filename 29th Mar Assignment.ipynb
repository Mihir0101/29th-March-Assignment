{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b70852e0-5bc8-4f5c-8c9d-067e6f39ec2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 29th March Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f88617f-1122-463a-9fe8-27ccfa9ac791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f05b55b3-8532-4c1f-ba4d-ee1e2d26166c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98112e55-5898-4c7f-a59a-ca704ca67e90",
   "metadata": {},
   "source": [
    "- > It is an type of regression model.\n",
    "\n",
    "- > It is used for feature selection.\n",
    "\n",
    "- > Lasso regression add some parameters in cost function.\n",
    "\n",
    "- > That new cost function helps us to select the most important features.\n",
    "\n",
    "- > Parameters are : Hyperparameter and Coefficient.\n",
    "\n",
    "- > When we increase the value of hyperparameter the value of coefficient will decrease.\n",
    "\n",
    "- > We have to decrease coefficients according to the threshold values.\n",
    "\n",
    "- > If any coefficient becomes 0(zero),so we will remove that perticular feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7014bee1-eae3-43fd-88d0-7f26a5a87045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "029f29f0-7dfa-44c5-aa76-434efe162e24",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbdb2d6-76be-4a77-bf15-77ff153545f2",
   "metadata": {},
   "source": [
    "- > Lasso regression adds penalty turm in cost function.\n",
    "\n",
    "- > While the process of optimization,Lasso tends to force coefficient of some features to be exactly zero.\n",
    "\n",
    "- > This sparsity-inducing property is beneficial for feature selection because it effectively leads to the automatic selection of a subset of features, discarding the less important ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb107a6-6c39-4fc6-8b90-fd483914e434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8f5d194-a9ad-4166-90c0-422520e4faff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad9470e-da6a-4715-a19c-b0a822540e43",
   "metadata": {},
   "source": [
    "* Interpreting the coefficients of lasso regression includes understanding impace of each feature on dependent feature.\n",
    "\n",
    "- > The sign of the coefficient indicates the direction of the relationship. A positive coefficient suggests a positive impact on the dependent variable, while a negative coefficient suggests a negative impact.\n",
    "\n",
    "- > The magnitude of non-zero coefficients provides information about the strength of the relationship between the corresponding features and the dependent variable.\n",
    "\n",
    "- > Lasso can handle multicollinearity to some extent by selecting one of the correlated features and assigning it a non-zero coefficient while setting the coefficients of the others to zero.\n",
    "\n",
    "- > The intercept term in a Lasso model represents the estimated value of the dependent variable when all input features are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656dd74d-66bc-4a89-b20a-2bec4bae457d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76ffeab8-1dae-4bf7-b0b9-6e1871e17494",
   "metadata": {},
   "source": [
    "## Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9f71bd-0211-48e2-9258-3ac984983edf",
   "metadata": {},
   "source": [
    "- > The main tuning parameter is the regularization strength, often denoted as λ (lambda) or α (alpha). This parameter controls the amount of regularization applied to the model. The regularization term is added to the ordinary least squares (OLS) loss function to prevent overfitting and encourage sparsity in the coefficients.\n",
    "\n",
    "- > Choosing the right λ value is crucial for model performance. Commonly, cross-validation techniques, such as k-fold cross-validation, are used to assess the model's performance for different λ values. The λ that yields the best performance on a validation set is typically selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c881b90e-7e22-4d09-8a39-92178aed1e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96e398f8-2541-4260-a974-d57374ea6d8a",
   "metadata": {},
   "source": [
    "## Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eed216-eda2-4831-83fd-ade60e6dcc2b",
   "metadata": {},
   "source": [
    "- > Yes.\n",
    "\n",
    "- > Basically lasso regression is designed to solve the linear problems.\n",
    "\n",
    "- > But we can use it for non-linear relationship through some techniques.\n",
    "\n",
    "- > We can you it for polynomial relationship by squaring and cubing the features.\n",
    "\n",
    "- > Another approach involves using the kernel trick to implicitly map the input features into a higher-dimensional space. This is often seen in support vector machines and other kernelized methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2f862d-ee74-48fa-8878-f3423afe191c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4a6c016-9ef8-4e02-b6ce-8043b52aca71",
   "metadata": {},
   "source": [
    "## Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e8bdc5-422d-4cd1-9d73-f509a0ac92d3",
   "metadata": {},
   "source": [
    "* Use\n",
    "\n",
    "Ridge : It is used to reduce the overfitting.\n",
    "\n",
    "Lasso : It is used for feature selection.\n",
    "\n",
    "* Also Known as\n",
    "\n",
    "Ridge : It is also known as L2 Regularization.\n",
    "\n",
    "Lasso : It is also known as L1 Regularization.\n",
    "\n",
    "* Cost Function\n",
    "\n",
    "Ridge : 1/n ∑ (Yi - Ŷ) ** 2 + λ ∑ (Slop) ** 2\n",
    "\n",
    "Lasso : 1/n ∑ (Yi - Ŷ) ** 2 + λ ∑ |Slop| ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b13a1ab-c067-4f39-9c3b-dfd98b76be51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c1e2fe7-9fa2-481e-8e6a-d2c17fdabf96",
   "metadata": {},
   "source": [
    "## Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36b654f-85be-4f35-80b4-c631406a8c9a",
   "metadata": {},
   "source": [
    "- > Yes, Lasso Regression has the ability to handle multicollinearity in the input features, to some extent. Multicollinearity occurs when two or more independent variables in a regression model are highly correlated, making it difficult to separate their individual effects on the dependent variable. \n",
    "\n",
    "- > In the presence of multicollinearity, Lasso may select one variable from a group of highly correlated variables and assign it a non-zero coefficient, while setting the coefficients of the other variables in the group to zero.\n",
    "\n",
    "- > This penalty tends to shrink the coefficients of correlated variables, making them smaller and potentially more interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa674496-900a-4ae5-a3fe-383499800620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "777e7b7b-d815-4b88-b83c-43e3b78ec7e2",
   "metadata": {},
   "source": [
    "## Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e447e7-ac92-42d9-bc41-0fef21117042",
   "metadata": {},
   "source": [
    "- > There is two techniques I would use for find the best lambda value.\n",
    "\n",
    "- > That two techniques are : Cross Validation and Hyperparameter Tuning.\n",
    "\n",
    "- > I would use GridSearchCV or RandomizedSearchCV for that.\n",
    "\n",
    "- > Cross validation devides trainig dataset into training and validation part.\n",
    "\n",
    "- > Hyperparameter tuning make "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
